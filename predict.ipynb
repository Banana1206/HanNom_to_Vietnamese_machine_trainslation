{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from argparse import ArgumentParser\n",
    "# from preprocessing import remove_punctuation\n",
    "# from metric import CustomSchedule\n",
    "from train import Seq2Seq\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from Encoder_Attention_decoder import Encode, Attention, Decoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class predictionSentence(Seq2Seq):\n",
    "#     def __init__(self, embedding_size=64, hidden_units=256, learning_rate=0.001, test_split_size=0.1, epochs=10, batch_size=128):\n",
    "#         super().__init__(embedding_size, hidden_units, learning_rate, test_split_size, epochs, batch_size)\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module_no_signatures_path = os.path.join(tmpdir, 'module_no_signatures')\n",
    "# module(tf.constant(0.))\n",
    "# print('Saving model...')\n",
    "# tf.saved_model.save(module, module_no_signatures_path)\n",
    "\n",
    "# imported = tf.saved_model.load(module_no_signatures_path)\n",
    "# assert imported(tf.constant(3.)).numpy() == 3\n",
    "# imported.mutate(tf.constant(2.))\n",
    "# assert imported(tf.constant(3.)).numpy() == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from preprocessing import DatasetLoader, remove_punctuation_viet, add_space_nom\n",
    "from argparse import ArgumentParser\n",
    "from evaluation import evaluation, evaluation_with_attention\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from Encoder_Attention_decoder import Encode, Attention, Decoder\n",
    "from metric import Bleu_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "url = 'data/NomNaNMT.csv'\n",
    "inp_tensor, tar_tensor, inp_builder, tar_builder = DatasetLoader(url=url).build_dataset()\n",
    "\n",
    "# Initialize Seq2Seq model\n",
    "input_vocab_size = len(inp_builder.word_index) + 1\n",
    "target_vocab_size = len(tar_builder.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __preprocess_input_text__( text):\n",
    "    inp_lang =add_space_nom(text)\n",
    "    tokenize_inp = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenize_inp.fit_on_texts(inp_lang)\n",
    "    inp_vector = tokenize_inp.texts_to_sequences(inp_lang)\n",
    "    max_length = 20\n",
    "    inp_vector = pad_sequences(inp_vector, maxlen=max_length, padding='post')\n",
    "    return inp_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_lang =  df['Nom'][:4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大越史記外紀全書卷之一'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "inp_lang =add_space_nom(inp_lang)\n",
    "[[inp_builder.word_index[w] for w in inp_lang.split() if w in list(inp_builder.word_index.keys())]]\n",
    "# inp_vector = tokenize_inp.texts_to_sequences(inp_lang)\n",
    "# max_length = 20\n",
    "# inp_vector = pad_sequences(inp_vector, maxlen=max_length, padding='post')\n",
    "# tokenize_inp \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "13\n",
      "372\n",
      "116\n",
      "696\n",
      "100\n",
      "379\n",
      "515\n",
      "81\n",
      "852\n",
      "3\n",
      "22\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for w in inp_lang.split():\n",
    "    # print(w)\n",
    "    if w in list(inp_builder.word_index.keys()):\n",
    "        print(inp_builder.word_index[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = tar_builder.word_index.keys()\n",
    "value = tar_builder.word_index.values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = list(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hữu'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key[a[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp =  df[['Nom','Viet']][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大越史記外紀全書卷之一\n",
      "Đại Việt Sử Ký Ngoại Kỷ Toàn Thư Quyển Chi Nhất.\n",
      "朝列大夫國子監司業兼史官修撰臣吳士連編\n",
      "Triều liệt đại phu, Quốc Tử Giám Tư nghiệp, kiêm Sử quan tu soạn, thần Ngô Sĩ Liên biên.\n",
      "按黄帝時建萬國以交趾界於西南遠在百粤之表\n",
      "Án: Hoàng đế thời, kiến vạn quốc, dĩ Giao Chỉ giới ư Tây Nam, viễn tại Bách Việt chi biểu.\n",
      "堯命𦏁氏宅南交定南方交趾之地\n",
      "Nghiêu mệnh Hi thị trạch Nam Giao, định Nam phương Giao Chỉ chi địa.\n"
     ]
    }
   ],
   "source": [
    "for i, lab in zip(df['Nom'][:4],df['Viet'][:4]) :\n",
    "    print(i)\n",
    "    print(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x1897e1e25e8>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Seq2Seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bb4085df3d0e855489bbfd8fbbf310942c04af6f68564c9c5237a442f6d91b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
